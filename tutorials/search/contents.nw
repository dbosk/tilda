\mode*

\section{Rabin--Karp}

Det vi vill göra är att söka efter en textsträng~[[search_term]] i en längre 
textsträng~[[text]].
Vi vill använda en implementation av Rabin-Karps algoritm för sökning, 
[[rk_search]].
Denna ska returnera index där strängen börjar.
\begin{frame}[fragile]
<<rk.py>>=
<<definition Rabin-Karp search function>>

def main():
  text = "TEMAT ÄR TELETEMA"

  for term in ["TEMA", "TEMATISK", "TELETEMA"]:
    try:
      print(f"{term} is in {text} at position {rk_search(term, text)}")
    except IndexError as err:
      print(err)

if __name__ == "__main__":
  main()
@
\end{frame}

Om vi kör koden
\begin{pyblock}
import rk
rk.main()
\end{pyblock}
kommer vi att få följande resultat:
\begin{frame}[fragile]
\stdoutpythontex[verbatim]
\end{frame}

\subsection{Sökning med Rabin-Karps algoritm}

Hur ser då Rabin-Karps algoritm~\cite{RabinKarpSearch} ut?
Vi behöver en hashfunktion, och gärna en rullande sådan.
En rullande hashfunktion har en speciell egenskap.
Låt oss säga att vi har en sträng~\[
  s_{[1, n]} = s_1s_2\dotsb s_n
\] av längd \(n\), där varje \(s_i\) är en bokstav.
Vi kan beräkna hashvärdet~\[
  h_i = h(s_{[i, i+m]})
\] för en delsträng~\(
  s_{[i, i+m]}
\) av längd \(m\) med tidskomplexitet \(O(m)\).
Om \(h\) är rullande kan vi beräkna \[
  h_{i+1} = h(s_{[i+1, i+m+1]})
\] med tidskomplexitet \(O(1)\) med hjälp av \[
  h_i,\qquad s_i\qquad \text{och}\qquad s_{i+m+1}.
\]

This leaves us with the following Python functions.
\begin{frame}[fragile]
<<definition Rabin-Karp search function>>=
def rk_hash(text):
  """Computes hash value for text"""
  <<compute hash of text>>

def rk_hash_next(prev_symbol, prev_hash, next_symbol):
  """Computes rk_hash for the next string, by removing prev_symbol and adding 
  next_symbol."""
  <<compute the next hash from the previous>>

def rk_search(term, text):
  """Returns index of first occurence of term in text"""
  <<perform Rabin-Karp search for term in text>>
@
\end{frame}

Låt oss titta på hur dessa bör se ut.
Vi börjar med att spara undan längderna av strängarna, annars är kostnaden 
\(O(m)\) respektive \(O(n)\).
Om vi inte hittar strängen vi letar efter så kastar vi ett särfall.
\begin{frame}[fragile]
<<perform Rabin-Karp search for term in text>>=
term_length = len(term) # m
text_length = len(text) # n

<<compute the initial hashes>>
<<compute the next hash for every substring, check if match>>

raise IndexError(f"{term} not found in {text}")
@
\end{frame}

Vi behöver beräkna hasharna för söktermen och den första delsträngen.
Båda dessa kräver \(O(m)\) i komplexitet.
Dessutom kopierar vi en sträng genom Pythons slicing ([[text[0:term_length]]]), 
denna operation kräver också \(O(m)\).
När vi har beräknat hasharna kan vi kolla om de är lika, det kan ju hända att 
strängen vi söker efter är den första delsträngen.
\begin{frame}[fragile]
<<compute the initial hashes>>=
# O(m)
target_hash = rk_hash(term)

# O(m)
subtext = text[0:term_length]
# O(m)
subtext_hash = rk_hash(subtext)

if subtext_hash == target_hash and term == text[0:term_length]:
  return 0
@
\end{frame}

Sedan måste vi söka igenom texten.
Detta ger en tidskomplexitet på \(O(n)\), för vi måste gå igenom hela 
textsträngen tecken för tecken.
Här använder vi egenskapen att hashfunktionen är rullande ([[rk_hash_next]]), 
detta ger oss att vi kan beräkna hashen i \(O(1)\).
\begin{frame}[fragile]
<<compute the next hash for every substring, check if match>>=
# O(n)
for i in range(1, text_length-(term_length-1)):
  # O(1)
  subtext_hash = rk_hash_next(text[i-1], subtext_hash, text[i+term_length-1])
  # O(1) and O(m)
  if subtext_hash == target_hash and term == text[i:i+term_length]:
    return i
@
\end{frame}
Om vi istället hade använt [[rk_hash]], då hade vi fått \(O(m)\) för 
\emph{varje varv}.
Det hade resulterat i \(O(nm)\) totalt, men tack vare rullningen får vi 
\(O(n\cdot 1)\) istället.

Om vi tittar på [[if]]-satsen ser vi att vi har ytterligare en \(O(m)\) 
operation, jämförelsen av två textsträngar.
Men denna operation genomförs endast i de fall hashvärdena är lika.
Så med en dålig hashfunktion (eller beroende på söksträngen och texten) händer 
det ofta (närmare \(O(nm)\) totalt), med en bättre hashfunktion händer det mer 
sällan (närmare \(O(n+m)\) totalt).

Om vi tittar på det hela taget, då får vi \(O(3m)\) från
[[<<compute the initial hashes>>]] och \(O(n+m)\) eller \(O(nm)\) från
[[<<compute the next hash for every substring, check if match>>]].
Sammantaget blir det alltså \(O(3m+n+m) = O(m+n)\) eller \(O(3m+nm) = O(nm)\).

\subsection{Rullande hashning}

Hur kan vi då implementera rullande hashning och åstadkomma \(O(1)\)?
För att göra detta bra krävs en del algebraiska egenskaper.
Ett exempel på en sådan metod är Rabins 
fingeravtrycksalgoritm~\cite{RabinFingerprinting}.
Vi ska dock använda en enklare rullande hashfunktion.

Vår enkla algoritm går ut på att bara summera symbolerna: \[
  \sum_{x \in [i, i+m]} s_x.
\]
Vi ser att denna kräver \(O(m)\) i komplexitet.
\begin{frame}[fragile]
<<compute hash of text>>=
return sum(ord(symbol) for symbol in text)
@
\end{frame}
Tack vare kommutativiteten hos addition av heltal kommer vi att få många 
kollisioner, exempelvis
\begin{pyblock}
import rk

print(f"rk_hash(TEST) = {rk.rk_hash('TEST')}")
print(f"rk_hash(ESTT) = {rk.rk_hash('ESTT')}")
\end{pyblock}
ger en kollision:
\begin{frame}[fragile]
\stdoutpythontex[verbatim]
\end{frame}
Denna hashfunktion kommer med andra ord göra att vår Rabin--Karp-sökning inte 
blir optimal (\(O(n+m)\)), men ändå inte nödvändigtvis så dålig som värsta 
fallet (\(O(nm)\)).

När vi nu vill beräkna nästkommande hashvärde, kan vi göra det enkelt.
Vi har att hashvärdena för \(s_{[i, i+m]}\) och \(s_{[i+1, i+1+m]}\) skiljer 
sig med termerna \(s_i\) och \(s_{i+1+m}\).
Tack vare att addition är kommutativ (och associativ), kan vi bara dra bort den 
ena och lägga till den andra.
Detta är en konstant operation, helt oberoende av \(m\).
\begin{frame}[fragile]
<<compute the next hash from the previous>>=
return prev_hash - ord(prev_symbol) + ord(next_symbol)
@
\end{frame}

Vi kan testa dessa funktioner med följande kod.
\begin{pyblock}
import rk

test_hash = rk.rk_hash('TEST')
print(f"rk_hash(TEST) = {test_hash}")
print(f"rk_hash(ESTA) = {rk.rk_hash('ESTA')}")
next_hash = rk.rk_hash_next('T', test_hash, 'A')
print(f"rk_hash_next('T', {test_hash}, 'A') = {next_hash}")
\end{pyblock}
Det ger följande resultat.
\begin{frame}[fragile]
\stdoutpythontex[verbatim]
\end{frame}


\section{Boyer--Moore}

Idéen bakom Boyer-Moores algoritmen är att göra större hopp igenom texten än 
med en brute-force metod genom att jämföra den sökta textsträngen~[[pattern]] 
med en lämgre textsträng~[[text]] från höger till vänster. Ett viktigt steg i 
metoden är att bestämma en KMP-automat innan vi börjar för att kunna avgöra i 
\(O(1)\) det största steget framåt vi får ta vid en missmatch. Detta 
preprocessing kan optimeras så mycket så att vi får en tidskomplexitet av 
\(O(n)\) för hela sökningen i värsta fallet. Vi kan här dock fokusera på den 
ursprungliga implementationen~\cite{BoyerMoore} och låta den nyfikna läsaren 
undersöka~\cite{GalilRule} och~\cite{ApostolicoGiancarlo}.

Vi behöver först analysera den sökta strängen och bestämma automaten och 
speciellt sin \(next\)-array representation. Tänk att vi letar efter strängen 
\enquote{TEMATISK} i texten \enquote{TEMAT ÄR TELETEMA}. Vid första jämförelsen 
kommer K:et från \enquote{TEMATISK} att jämföras med R:et i texten. Eftersom 
det inte finns något R i den sökta strängen --- det har vi kodat i automaten 
--- kan vi direkt hoppa över åtta tecken (längden på söksträngen).

Med tanke på att vi behöver anpassa oss efter söktermen är det lämpligt att vi 
har följande klasstruktur.
\begin{frame}[fragile]
<<bm.py>>=
class BoyerMoore:
  """Object to use for searching for a specific term in varying texts"""
  def __init__(self, pattern):
    """We construct the object for the term to search for, pattern"""
    <<preprocessing pattern>>

  def search_in(self, text):
    """Search through text for the pattern used in the constructor"""
    <<use next-array to make as big jumps as possible>>
@
\end{frame}
Detta betyder att vi kan återanvända sökobjektet för att söka efter samma term 
i flera olika texter.
    

\subsection{Initiering av Boyer--Moore}

Mer konkret vill vi ha en heltalsvektor lika lång som alfabetet vi jobbar med, 
där ett element har värdet -1 om motsvarande tecken i alfabetet inte finns i 
den sökta strängen, eller indexen till det första tillfället av samma bokstav i 
strängen från höger (eftersom att vi börjar i slutet av orden).
Låt oss se i detalj vad som händer inom konstruktorn.
Tecken som finns i [[pattern]] mappas till index av första tillfället från 
höger.
\begin{frame}[fragile]
<<preprocessing pattern>>=
alphabet_size = 256 # assumption: extended ASCII only

self.pattern = pattern

self.next = [-1]*alphabet_size # initialize all to -1
# O(m)
for i, alfa in enumerate(pattern):
  self.next[ord(alfa)] = i
@
\end{frame}
(Detta betyder att komplexiteten för detta blir \(O(m)\), då vi måste gå igenom 
hela söksträngen av längd \(m\).)
\begin{frame}[fragile]
Vi kan undersöka next-vektorn för ett exempel.
\begin{pyblock}
import bm

search_obj = bm.BoyerMoore("TEMATISK")

for character, next in enumerate(search_obj.next):
  if next != -1:
    print(f"{chr(character)}: {next}", end="    ")
\end{pyblock}
Utmatningen blir följande:
\stdoutpythontex[verbatim]
\end{frame}

\subsection{Sökning med Boyer--Moore}

I vår enkla implementation, inspirerad från~\cite{Sedgewick}, måste vi 
fortfarande kolla på ett specialfall. Om första förekomsten av bokstaven vid en 
felmatchning befinner sig till höger om det felmatchade tecknet tillåter vi 
inte en hopp åt fel håll. Då ska vi istället skifta mönstret ett steg till 
höger. Notera att detta kan vi behandla på samma sätt som tecken som inte finns 
i [[pattern]], eftersom allt initierades med -1.
\begin{frame}[fragile]
<<use next-array to make as big jumps as possible>>=
n = len(text)
m = len(self.pattern)

i = 0
# start from left (beginning), stop suitably before the end
# O(n-m)
while i <= n-m:
  jump = 0
  # start comparing from the right (end)
  # O(1) to O(m) depending on data
  for j in range(m-1, 0, -1):
    if self.pattern[j] != text[i+j]:
      # O(1)
      jump = j - self.next[ord(text[i+j])]
      if jump < 1:
        jump = 1
        break

  if jump == 0:
    return i # hittade strängen
  i += jump

raise IndexError(f"{self.pattern} not in {text}")
@
\end{frame}
Notera att \(j = m-1\) vid början.
Om vi inte hittar någon matchning, då kommer vi att hoppa \(m\) steg eftersom 
att \(j-(-1) = (m-1)-(-1)\) då blir \(m\).
Därför sätter vi \(-1\) på alla bokstäver som inte finns i vår sökterm.

Vad blir då komplexiteten?
Vi har en iteration ([[while]]-satsen) som går \(O(n-m)\).
För varje sådan iteration har vi [[for]]-satsen.
I de flesta fall jämför denna sista bokstaven (\(O(1)\)),
kollar [[next]]-vektorn (\(O(1)\)),
hoppar framåt (går till nästa varv i [[while]]-satsen).
Det är bara i de fall som vi har matchningar som denna [[for]]-sats kommer att 
bli mellan \(O(1)\) och \(O(m)\).
Men vi får åtminstone en \(O(m)\) för denna [[for]]-sats, nämligen när vi får 
en fullträff på sökordet.
Om vi inte får några felmatchningar får vi då \(O(n-m+m)\), d.v.s. \(O(n)\).
Får vi däremot delvisa matchningar hela tiden, då får vi \(O(nm)\).

\begin{frame}[fragile]
Vi kan testa att denna algoritm funkar på vårt exempel.
\begin{pyblock}
import bm

text = "TEMAT ÄR TELETEMA"

for term in ["TEMA", "TEMATISK", "TELETEMA"]:
  term_obj = bm.BoyerMoore(term)
  try:
    print(f"{term} is in {text} at position {term_obj.search_in(text)}")
  except IndexError as err:
    print(err)
\end{pyblock}
Utmatningen blir följande:
\stdoutpythontex[verbatim]
\end{frame}
